{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import torch and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "import sys\n",
    "import networkx as nx\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SNAP and BioSNAP specific scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepsnap\n",
    "from converter import *\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from deepsnap.batch import Batch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process BioSNAP Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See converter.py for more details on the datasets, and how conversion occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data/'\n",
    "name = 'BioSNAP-Chemical-Chemical'\n",
    "f = datadir + 'minercc.tsv'\n",
    "f2 = datadir + 'minerc.tsv'\n",
    "d = readFilePD(f)\n",
    "d2 = readFilePD(f2,['type'])\n",
    "# label node feature as 'node feature'\n",
    "nxg = pdToNxCC(d,d2)\n",
    "dg = deepsnap.graph.Graph(nxg)\n",
    "graphs = dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset and loader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(graphs, task='node')  \n",
    "dataset_train, dataset_val, dataset_test = dataset.split(\n",
    "    transductive=True,\n",
    "    split_ratio=[0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(),\n",
    "                          batch_size=16)  \n",
    "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(),\n",
    "                        batch_size=16) \n",
    "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(),\n",
    "                         batch_size=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.conv1 = GCNConv(dataset.num_node_features, 1)\n",
    "        # self.conv2 = GCNConv(16, dataset.num_node_labels)\n",
    "        self.conv1 = SplineConv(1, 16, dim=1, kernel_size=2)\n",
    "        self.conv2 = SplineConv(16, 4, dim=1, kernel_size=2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index, edge_feature = \\\n",
    "            batch.node_feature, batch.edge_index, batch.edge_feature\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_feature))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_feature)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for batch in train_loader:\n",
    "        batch.to(device)\n",
    "        emb = model(batch)\n",
    "        loss = F.nll_loss(emb[batch.node_label_index],\n",
    "                          batch.node_label[batch.node_label_index])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for loader in [train_loader, val_loader, test_loader]:\n",
    "        for batch in loader:\n",
    "            batch.to(device)\n",
    "            logits = model(batch)\n",
    "            pred = logits[batch.node_label_index].max(1)[1]\n",
    "            acc = pred.eq(batch.node_label[batch.node_label_index]).sum().item()\n",
    "            total = batch.node_label_index.shape[0]\n",
    "            acc /= total\n",
    "            accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_max = -math.inf\n",
    "best_model = model\n",
    "trainR = []\n",
    "valR = []\n",
    "testR = []\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(log.format(epoch, train_acc, val_acc, test_acc))\n",
    "    trainR.append(train_acc)\n",
    "    valR.append(val_acc)\n",
    "    testR.append(test_acc)\n",
    "    if val_max < val_acc:\n",
    "        val_max = val_acc\n",
    "        # best_model = copy.deepcopy(model)\n",
    "\n",
    "# model = best_model\n",
    "log = 'Best, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "train_acc, val_acc, test_acc = test()\n",
    "print(log.format(train_acc, val_acc, test_acc))\n",
    "trainR.append(train_acc)\n",
    "valR.append(val_acc)\n",
    "testR.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trainR)\n",
    "plt.plot(valR)\n",
    "plt.plot(testR)\n",
    "plt.show()\n",
    "plt.savefig('nodeClassificationCC.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Drug Approval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the drugbank data, we can also predict the approval of drugs, since we are given information about drug relationships and types. We must mask information about drug withdrawl as this would imply no approval, and would make the task trivial and meaningless. The task is as follows, given a drugs neighbors, type information, and information about whether it is investigational, illicit, and/or experimental, we predict 0 (not approved) or 1 (approved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt classying drug approval\n",
    "# lbl can be 'approved','illicit','investigational','withdrawn','experimental', 'nutraceutical'\n",
    "\n",
    "# mask information about whether a drug is withdrawn\n",
    "toMask = ['withdrawn']\n",
    "# one can also mask illicit, as it may also provide extra information that helps the model overperform\n",
    "\n",
    "nxg = pdToNxCC(d,d2,lbl = 'approved', mask = toMask)\n",
    "dg = deepsnap.graph.Graph(nxg)\n",
    "graphs = dg\n",
    "\n",
    "dataset = GraphDataset(graphs, task='node')  # node, edge, link_pred, graph\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(\n",
    "    transductive=True,\n",
    "    split_ratio=[0.8, 0.1, 0.1])  # transductive split, inductive split\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(),\n",
    "                          batch_size=16)  # basic data loader\n",
    "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(),\n",
    "                        batch_size=16)  # basic data loader\n",
    "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(),\n",
    "                         batch_size=16)  # basic data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "# set hyperparameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "val_max = -math.inf\n",
    "best_model = model\n",
    "trainR = []\n",
    "valR = []\n",
    "testR = []\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(log.format(epoch, train_acc, val_acc, test_acc))\n",
    "    trainR.append(train_acc)\n",
    "    valR.append(val_acc)\n",
    "    testR.append(test_acc)\n",
    "    if val_max < val_acc:\n",
    "        val_max = val_acc\n",
    "        # best_model = copy.deepcopy(model)\n",
    "\n",
    "# model = best_model\n",
    "log = 'Best, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "train_acc, val_acc, test_acc = test()\n",
    "print(log.format(train_acc, val_acc, test_acc))\n",
    "trainR.append(train_acc)\n",
    "valR.append(val_acc)\n",
    "testR.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainR)\n",
    "plt.plot(valR)\n",
    "plt.plot(testR)\n",
    "plt.show()\n",
    "plt.savefig('nodeClassificationCCapprov.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Drug Withdrawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can predict drug withdrawl. This time we don't have to mask any information, since even approved drugs can be withdrawn. In fact any drug can be withdrawn for a number of reasons, this may explain the lack of accuracy in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toMask = ['withdrawn']\n",
    "\n",
    "nxg = pdToNxCC(d,d2,lbl = 'withdrawn', mask = toMask)\n",
    "dg = deepsnap.graph.Graph(nxg)\n",
    "graphs = dg\n",
    "\n",
    "dataset = GraphDataset(graphs, task='node')  # node, edge, link_pred, graph\n",
    "dataset_train, dataset_val, dataset_test = dataset.split(\n",
    "    transductive=True,\n",
    "    split_ratio=[0.8, 0.1, 0.1])  # transductive split, inductive split\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(),\n",
    "                          batch_size=16)  # basic data loader\n",
    "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(),\n",
    "                        batch_size=16)  # basic data loader\n",
    "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(),\n",
    "                         batch_size=16)  # basic data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "# set hyperparameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "val_max = -math.inf\n",
    "best_model = model\n",
    "trainR = []\n",
    "valR = []\n",
    "testR = []\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(log.format(epoch, train_acc, val_acc, test_acc))\n",
    "    trainR.append(train_acc)\n",
    "    valR.append(val_acc)\n",
    "    testR.append(test_acc)\n",
    "    if val_max < val_acc:\n",
    "        val_max = val_acc\n",
    "        # best_model = copy.deepcopy(model)\n",
    "\n",
    "# model = best_model\n",
    "log = 'Best, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "train_acc, val_acc, test_acc = test()\n",
    "print(log.format(train_acc, val_acc, test_acc))\n",
    "trainR.append(train_acc)\n",
    "valR.append(val_acc)\n",
    "testR.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainR)\n",
    "plt.plot(valR)\n",
    "plt.plot(testR)\n",
    "plt.show()\n",
    "plt.savefig('nodeClassificationCCwithdrawl.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
